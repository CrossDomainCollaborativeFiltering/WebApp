<h1 id="non-numeric-data-clustering">Non-Numeric Data Clustering</h1>
<h2 id="tf-idf-vectorization-">tf-idf Vectorization :</h2>
<p>Before we go into depth of tf-idf vectorization, a little background on tf-idf weighting metric.</p>
<h3 id="term-frequency-tf-">Term frequency(tf):</h3>
<p>The term frequency is simply the number of times a term occurs in a particular document. It is measure of scoring a term in a document. The log based term frequency metric is the following:</p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/tf.PNG" alt="Log Based Term frequency"> </p>
<p>The term frequency matching scoring: </p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/tfMatchingScore.PNG" alt="Term frequency matching score"> </p>
<p>The term frequency matching score tells us the rank of a query term in a document such that the query term actually exists in the document. i.e. q (intersection) d is not empty.</p>
<h3 id="inverse-document-frequency-idf-">Inverse Document Frequency(idf):</h3>
<p>The document frequency is the number of documents in the collection where the term occurs. It is an important mertic as it gives priority to terms occur only sparsly in the document. The collection frequency or the number of times a term appears in the entire collection could been a good metric however as it turns out it is actually not. To accurately measure the sparsity of a term therefore it is important to mesaure the number of documents the term occurs in and inverting that can give us a good measure of the rare terms in our document and hence the terms that carry more relevance and hence more weight.</p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/idf.PNG" alt="The log based inverse document frequency"></p>
<h3 id="tf-idf-weighting-">tf-idf weighting:</h3>
<p>The tf-idf weighting takes care of both the more frequent terms and the rare terms in a collection considering rare terms or terms occuring in less number of documents but more often in a single document should have more weight and should rank higher than the ones which are either too less in a document or spread over large number of documents.</p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/tf-idf.PNG" alt="tf-idf weighing scheme"></p>
<p>Therefore in general for a query phrase or a set of query terms:</p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/tfidfQueryScoring.PNG" alt="tf-idf query scoring"> </p>
<h2 id="vector-space-model-vsm-">Vector Space Model(VSM):</h2>
<p>In Information retrieval and Text mining vector space model is a widely used tool to cluster similar terms in a collection together and perform other operations on them.</p>
<p>The representation of a set of documents as a vector in a common vector space is known as the vector space model and is fundamental to a host of information retrival operations ranging from scoring documents from a query, document classification and document clustering.</p>
<p>We denote the vector V derived from a document d with one component in the vector for each dictionary term. Unless otherwise specified, the reader may simply assume that components are computed using the tf-idf weighting scheme. The set of documents in a collection then may be viewed as a set of vectors in a vector space where each dimension or each axis of each of the vectors represent one query term.</p>
<h3 id="computing-document-similarities-">Computing document similarities:</h3>
<p>Once all documents are represented in the form of vectors, a similarity metric can be formulated in order to cluster documents of similar types.</p>
<p>The most naive approach would be to measure simlarity based on euclidean distance however that would not necessarily be a good measure. Here&#39;s how:</p>
<p>There can be 2 documents that are similar in a major way as they have equal proportions of terms in them however one has more <strong>number</strong> of terms in them than the other. Euclidean distance would classify them as distant documents however in reality they would be very similar.</p>
<p>The standard similarity is a <strong>Cosine Similarity</strong>:</p>
<p>Given by: <img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/cosineSim.PNG" alt="Cosine Similarity"></p>
<p>This essentially computes a far in terms of angle is one document from another. The lower the angle the higher the consine and hence higher the similarity.</p>
<p>As we all know the dot product of 2 vectors is the scalar product of their magnitudes multiplied with the cosine of the angle between them. So if we wanna get the cosine all we need to do is the dot product divided by the scalar product of their magnitudes.</p>
<p>The following picture gives us an idea of how a query document can be used to figure our documents close to it.</p>
<p><img src="https://github.com/CrossDomainCollaborativeFiltering/Clustering/blob/master/assets/vsm.PNG" alt="VSM Model"></p>
<p>Here gossip and jealous are the only terms and d1 and d2 are the 2 documents. q is the query.</p>
<p>The more general way to think about cosine similarity is that we are firt normalizing the vectors and then computing the dot product in which case the magnitudes of the vectors are 1.</p>
<h2 id="application-of-vsm-in-movies-datasets-">Application of VSM in Movies Datasets:</h2>
<h5 id="for-vectorization-of-each-movie-in-the-dataset">for vectorization of each movie in the dataset</h5>
<p>In a movie based dataset the record of each movie can be considered as a  document and the dimensions of the vector becomes the tf-idf scores for the terms that occur in a document. </p>
<p>As a reference to how this concept can be used check out: <a href="http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/">this link</a></p>
<h4 id="python-implementation-">Python implementation:</h4>
<p>Scikit learn library has modules to do tfidf vectorization for us and thats what we are gonna use.</p>
<p>Lets say that we have the following training set:</p>
<p>trainSet=[&quot;The sky is blue&quot;,&quot;The sun is bright&quot;]</p>
<p>Based on the terms we can build a term document matrix in the following:</p>
<p>doc1: The sky is blue</p>
<p>doc2: The sun is bright</p>
<table>
<thead>
<tr>
<th></th>
<th>Term1</th>
<th>Term2</th>
<th>Term3</th>
<th>Term4</th>
</tr>
</thead>
<tbody>
<tr>
<td>Doc 1</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
</tr>
<tr>
<td>Doc 2</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
</tr>
<tr>
<td>Doc 3</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
<td>freq</td>
</tr>
</tbody>
</table>
<p>Each frequency actually represents the frequency of the terms in the corresponding document.</p>
<p>Here is a bit of code that can actually create a term document matrix for a particular dataset of documents</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer

v=CountVectorizer()

trainSet=[&quot;The sky is blue&quot;,&quot;The sun is bright&quot;,&quot;The sky soares high up in the sky&quot;]

termDocumentMatrix=v.fit_transform(trainSet)
print(termDocumentMatrix)
</code></pre><p>The output of the following document is:</p>
<pre><code>  (0, 0)        1
  (0, 4)        1
  (0, 5)        1
  (0, 8)        1
  (1, 1)        1
  (1, 7)        1
  (1, 4)        1
  (1, 8)        1
  (2, 3)        1
  (2, 9)        1
  (2, 2)        1
  (2, 6)        1
  (2, 5)        2
  (2, 8)        2
</code></pre><p>The above output actually represents the term document matrix in a form called the coordinate list notation. The representation is (row, column)  value.</p>
<p>In a similar fashion there are modules that help us convert whole collection of documents into tf-idf matrices.<br>The following code computes the idf for a term document matrix.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfTransformer

idf=TfidfTransformer(norm=&#39;l2&#39;)

idf.fit(termDocumentMatrix)

&quot;&quot;&quot;
here the parameter termDocumentMatrix is basically the term document matrix variable.
&quot;&quot;&quot;
</code></pre><p>Now the fit() method has calculated the idf for the matrix, lets transform this resulting matrix to the tfidf weighted matrix.</p>
<pre><code>
tf_idf_matrix=tfidf.transform(t)

print(&quot;printing the final tfidf matrix: \n&quot;)

print (tf_idf_matrix.todense())
</code></pre><p>The final matrix is the one printed in that last line. That final matrix is the tf idf matrix...Try these codes out in your system</p>
<h3 id="implementation-and-final-clustering-">Implementation and Final Clustering:</h3>
<p>Now after the tfidf Vectorization part its time to actually do clustering of the documents.<br>Now our dataset has both non-numeric and numeric data as well. So we cannot use all our paramters for tfidf vectorization. </p>
<p>So in each database we need to make a classification of both the non-numeric data and the numeric data. </p>
<p><strong>Kaggle Dataset:</strong></p>
<p><strong>Non-numeric data:</strong> color, director_name, actor_2_name, genres, actor_1_name, movie_title, actor_3_name, plot_keywords, language, country.</p>
<p><strong>MovieLens Dataset:</strong></p>
<p><strong>Non-numeric data:</strong> title, spanishTitle, genre,  actorName, country, directorName, location 1, location 2, location 3, location 4</p>
<h4 id="clustering-of-movies-using-non-numeric-data">Clustering of Movies using Non-Numeric Data</h4>
<ul>
<li><p><strong>Step 1</strong>: Since we are going to use all the data extensively for the purpose of clustering, for the MovieLens dataset it is necessary to join all the data as in that dataset, data is pratitioned into various groups.</p>
<p>This can be done with any ORM (Object Relational Mapper) integrated with Python like <strong>Peewee or SqlAlchemy</strong>.</p>
</li>
<li><p><strong>Step 2</strong>: Preprocessing of data. This step involves removal of stop words(if any) and converting each movie into a paragraph of terms(to be used for tfidf weighting).</p>
</li>
<li><p><strong>Step 3</strong>: tfidf weighting and conversion to VSM.</p>
</li>
<li><p><strong>Step 4</strong>: Clustering based on <strong>K Means</strong></p>
</li>
</ul>
